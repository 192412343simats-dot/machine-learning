{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8HiNlM52MPXYYKGfIk3v9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/192412343simats-dot/machine-learning/blob/main/3.%20ID3%20decision%20tree.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, attribute=None, value=None, results=None, true_branch=None,\n",
        "                 false_branch=None):\n",
        "        self.attribute, self.value, self.results, self.true_branch, self.false_branch = attribute, value, results, true_branch, false_branch\n",
        "def build_tree(rows):\n",
        "    if not rows:\n",
        "        return Node()\n",
        "    if len(set(row[-1] for row in rows)) == 1:\n",
        "        return Node(results=rows[0][-1])\n",
        "    num_attributes = len(rows[0]) - 1\n",
        "    # The best_attribute should be chosen based on the information gain of potential splits\n",
        "    # The current implementation chooses the attribute based on max information gain, which is correct.\n",
        "    # However, for splitting, we need a value for that attribute, not just the attribute index.\n",
        "    # Let's assume for now that the decision is binary ('Yes'/'No') for simplicity in `true_rows` and `false_rows`.\n",
        "    # This part needs refinement if attributes can have more than two values.\n",
        "    best_attribute = max(range(num_attributes), key=lambda col: information_gain(rows, col))\n",
        "\n",
        "    # For the decision tree, we need to pick a split value for the best attribute.\n",
        "    # The current code assumes a 'Yes'/'No' split directly on the best attribute, which might not be general.\n",
        "    # A more robust decision tree would iterate through unique values of the best_attribute to find the best split point.\n",
        "    # For this example, let's assume the attribute values are indeed 'Yes'/'No' or that we split based on the first value encountered for the attribute.\n",
        "    # A more correct implementation would find the optimal splitting value for the chosen attribute.\n",
        "\n",
        "    # For simplicity, assuming the 'value' to split on is the first value found for the best attribute.\n",
        "    # This part is a simplification and might not lead to an optimal tree for all datasets.\n",
        "    split_value = rows[0][best_attribute] # This is a placeholder; a real ID3 would find the optimal split value.\n",
        "\n",
        "    true_rows = [row for row in rows if row[best_attribute] == split_value] # Split based on 'split_value'\n",
        "    false_rows = [row for row in rows if row[best_attribute] != split_value] # Everything else\n",
        "\n",
        "    # If true_rows or false_rows are empty, it means no split occurred or one branch is empty.\n",
        "    # This can lead to infinite recursion if not handled. Add a check to prevent this.\n",
        "    if not true_rows or not false_rows:\n",
        "        return Node(results=class_counts(rows).most_common(1)[0][0]) # Return the majority class if no meaningful split\n",
        "\n",
        "    true_branch = build_tree(true_rows)\n",
        "    false_branch = build_tree(false_rows)\n",
        "    return Node(attribute=best_attribute, value=split_value, true_branch=true_branch,\n",
        "                false_branch=false_branch)\n",
        "def information_gain(rows, col):\n",
        "    total_entropy = entropy(rows)\n",
        "    values = set(row[col] for row in rows)\n",
        "    weighted_entropy = sum(len(list(filter(lambda row: row[col] == val, rows))) / len(rows) *\n",
        "                           entropy(list(filter(lambda row: row[col] == val, rows))) for val in values)\n",
        "    return total_entropy - weighted_entropy\n",
        "def entropy(rows):\n",
        "    from math import log2\n",
        "    # Handle case where rows is empty to prevent DivisionByZeroError or other issues\n",
        "    if not rows:\n",
        "        return 0.0\n",
        "    counts = class_counts(rows)\n",
        "    # Handle case where a count is 0, log2(0) is undefined. Filter out zero counts.\n",
        "    return -sum(count / len(rows) * log2(count / len(rows)) for count in counts.values() if count > 0)\n",
        "def class_counts(rows):\n",
        "    from collections import Counter\n",
        "    # Fix: rows.count(row) is incorrect, it should count the occurrences of the class label\n",
        "    # return dict((row[-1], rows.count(row)) for row in rows) # Original incorrect line\n",
        "    return Counter(row[-1] for row in rows) # Correct way to count class labels\n",
        "# Example dataset (you can modify this as needed)\n",
        "dataset = [\n",
        "    ['Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
        "    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
        "    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'Normal', 'Weak', 'Yes']\n",
        "]\n",
        "\n",
        "# Build the decision tree\n",
        "tree = build_tree(dataset)\n",
        "\n",
        "# Print the decision tree\n",
        "def print_tree(node, indent=\"\"):\n",
        "    if node is None:\n",
        "        return\n",
        "    if node.results is not None:\n",
        "        print(indent + str(node.results))\n",
        "    else:\n",
        "        # Original: print(indent + f'Attribute {node.attribute} : {node.value}? ')\n",
        "        # Fix: Need to access the attribute name, not just the index if we want more readable output\n",
        "        # For now, let's keep it as index for consistency with `attribute=best_attribute`\n",
        "        print(indent + f'Attribute {node.attribute} (Value: {node.value})? ')\n",
        "        print(indent + '--> True:')\n",
        "        print_tree(node.true_branch, indent + '  ')\n",
        "        print(indent + '--> False:')\n",
        "        print_tree(node.false_branch, indent + '  ')\n",
        "\n",
        "print_tree(tree)\n",
        "\n",
        "# Classify a new sample\n",
        "new_sample = ['Sunny', 'Cool', 'High', 'Strong']\n",
        "current_node = tree\n",
        "\n",
        "# Add a check for None in current_node before accessing attributes in the loop\n",
        "while current_node is not None and current_node.results is None and current_node.attribute is not None:\n",
        "    # The current classification logic assumes comparison with node.value\n",
        "    # A more robust classifier would follow the split condition defined during tree building.\n",
        "    if new_sample[current_node.attribute] == current_node.value:\n",
        "        current_node = current_node.true_branch\n",
        "    else:\n",
        "        current_node = current_node.false_branch\n",
        "\n",
        "# Handle cases where current_node becomes None during traversal (e.g., if a path doesn't exist)\n",
        "if current_node is None:\n",
        "    print(f\"\\nClassification result for {new_sample}: Could not classify (reached a None node)\")\n",
        "else:\n",
        "    print(f\"\\nClassification result for {new_sample}: {current_node.results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqwhRVAonjiQ",
        "outputId": "772dfa1c-abeb-4676-df32-4ff484d064eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attribute 0 (Value: Sunny)? \n",
            "--> True:\n",
            "  Attribute 1 (Value: Hot)? \n",
            "  --> True:\n",
            "    No\n",
            "  --> False:\n",
            "    Attribute 1 (Value: Mild)? \n",
            "    --> True:\n",
            "      No\n",
            "    --> False:\n",
            "      Yes\n",
            "--> False:\n",
            "  Attribute 3 (Value: Weak)? \n",
            "  --> True:\n",
            "    Yes\n",
            "  --> False:\n",
            "    Attribute 0 (Value: Rain)? \n",
            "    --> True:\n",
            "      No\n",
            "    --> False:\n",
            "      Yes\n",
            "\n",
            "Classification result for ['Sunny', 'Cool', 'High', 'Strong']: Yes\n"
          ]
        }
      ]
    }
  ]
}